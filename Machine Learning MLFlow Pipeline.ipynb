{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240d3a39-b029-4bd4-a2f4-2f8c78e98c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model with Hyperparameter Tuning:\n",
      "Top 20 Features: ['petal length (cm)', 'petal width (cm)', 'sepal length (cm)', 'sepal width (cm)']\n",
      "Best Hyperparameters: {'n_estimators': 50, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 10}\n",
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        10\n",
      "         1.0       1.00      1.00      1.00         9\n",
      "         2.0       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "data = np.c_[iris.data, iris.target]\n",
    "columns = np.append(iris.feature_names, [\"target\"])\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "def automated_pipeline(df, target_variable, features_to_predict):\n",
    "    # Select Features for modeling\n",
    "    df_model = df[features_to_predict + [target_variable]]\n",
    "\n",
    "    # Filter Category from underlying_index\n",
    "    cat_feature = [feature for feature in df_model.columns if df_model[feature].dtype == 'O']\n",
    "    numeric_features = [feature for feature in df_model.columns if df_model[feature].dtype != 'O']\n",
    "\n",
    "    # Handle missing values in categorical and numeric features\n",
    "    df_model[cat_feature] = df_model[cat_feature].fillna('OTHER')\n",
    "    df_model[numeric_features] = df_model[numeric_features].fillna(0)\n",
    "\n",
    "    # Step 2: Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_model.drop(target_variable, axis=1),\n",
    "                                                        df_model[target_variable],\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    # Step 3: Feature selection using Random Forest\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = pd.DataFrame(rf_model.feature_importances_,\n",
    "                                       index=X_train.columns,\n",
    "                                       columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Select top 20 features\n",
    "    top_features = feature_importances.head(20).index.tolist()\n",
    "\n",
    "    # Filter data with top features\n",
    "    X_train_selected = X_train[top_features]\n",
    "    X_test_selected = X_test[top_features]\n",
    "\n",
    "    # Step 4: Standard Scaling\n",
    "    scaler_standard = StandardScaler()\n",
    "    X_train_standard = scaler_standard.fit_transform(X_train_selected)\n",
    "    X_test_standard = scaler_standard.transform(X_test_selected)\n",
    "\n",
    "    # Step 5: Robust Scaling\n",
    "    scaler_robust = RobustScaler()\n",
    "    X_train_robust = scaler_robust.fit_transform(X_train_standard)\n",
    "    X_test_robust = scaler_robust.transform(X_test_standard)\n",
    "\n",
    "    # Step 6: Build a Random Forest model with hyperparameter tuning\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    rf_model_standard = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Using RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        rf_model_standard,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        cv=StratifiedKFold(n_splits=5),\n",
    "        scoring='accuracy',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Log parameters to MLflow\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"features_to_predict\", features_to_predict)\n",
    "        mlflow.log_param(\"target_variable\", target_variable)\n",
    "\n",
    "        # Train the model\n",
    "        random_search.fit(X_train_robust, y_train)\n",
    "\n",
    "        # Log hyperparameters to MLflow\n",
    "        mlflow.log_params(random_search.best_params_)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_standard = random_search.predict(X_test_robust)\n",
    "\n",
    "        # Step 7: Evaluate the model and log metrics to MLflow\n",
    "        accuracy = accuracy_score(y_test, y_pred_standard)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        print(\"Random Forest Model with Hyperparameter Tuning:\")\n",
    "        print(\"Top 20 Features:\", top_features)\n",
    "        print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred_standard))\n",
    "\n",
    "# Example usage:\n",
    "# Specify the target variable and features to predict\n",
    "target_variable = 'target'\n",
    "features_to_predict = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "\n",
    "# Call the automated pipeline function\n",
    "automated_pipeline(df, target_variable, features_to_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e8595-11e4-4853-9621-c00fec596fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
